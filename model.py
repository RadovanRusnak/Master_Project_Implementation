import random
import numpy as np
import matplotlib.pyplot as plt
from scipy.special import gammaln
import seaborn as sns
from sklearn.metrics import precision_recall_curve, roc_curve, auc
import math
import os
from config import CONFIG

np.random.seed(35)


def topol_order(incidence):
    """
    :param incidence: Incidence/adjacency matrix of a DAG
    :return: Topological order
    """
    n = len(incidence[0])
    Order = np.zeros(n, dtype=int)
    fan_in = np.zeros(n, dtype=int)

    for p in range(n):
        fan_in[p] = np.sum(incidence[:, p])

    no_fan_in = np.where(fan_in == 0)[0].tolist()

    m = 0
    while np.sum(Order == 0) > 0:
        next_node = no_fan_in[0]
        connected_nodes = np.where(incidence[next_node, :] == 1)[0]
        fan_in[connected_nodes] -= 1

        newly_zero_fan_in = np.where(fan_in == 0)[0]
        no_fan_in += [node for node in connected_nodes if node in newly_zero_fan_in and node not in no_fan_in]

        Order[m] = next_node + 1
        no_fan_in.pop(0)
        m += 1

    return Order


def get_parents_sets(adj_matrix):
    """
    :param adj_matrix: Incidence/adjacency matrix of a DAG
    :return: Parent configurations in adj_matrix
    """

    num_nodes = adj_matrix.shape[0]
    parents = {j: set() for j in range(num_nodes)}

    for j in range(num_nodes):
        for i in range(num_nodes):
            if adj_matrix[i][j] == 1:
                parents[j].add(i)

    return parents


def R_data(R, R_len, var=0.1):
    """
    :param R: True DAG network
    :param R_len: Size of each dataset
    :param var: Variance in linear Gaussian Eq. (3.1), default set to 0.1
    :return: Dataset generated by R - Eq. (3.1)
    """
    R_ordered = topol_order(R)
    R_parents = get_parents_sets(R).keys()
    D = np.zeros((R.shape[0], R_len))

    for i in R_ordered:
        for node in R_parents:
            if i - 1 == node:
                node_parents = get_parents_sets(R)[node]
                X_i = np.random.normal(0, var, len(D[i - 1]))
                for x_k in node_parents:
                    vals = np.random.uniform(0.5, 2, 1)
                    sign = np.random.choice([-1, 1], 1)
                    w = vals * sign
                    X_i = X_i + w * D[x_k, :]
                D[i - 1, :] = X_i

    return D


def child(edges, n):
    """
    :param edges: Edge array
    :param n: Number of nodes
    :return: Children array
    """
    p = np.ceil(edges / n).astype(int)
    return p


def parent(edges, n):
    """
    :param edges: Edge array
    :param n: Number of nodes
    :return: Parent array
    """
    ch = edges + n - child(edges, n) * n
    return ch


def order_edges(incidence):
    """
    :param incidence: Incidence/adjacency matrix
    :return: [edge, parent, child, position of the edge in the
    order]
    """
    top_ordering = topol_order(incidence)
    n = len(top_ordering)

    edges = np.where(incidence.flatten(order='F') != 0)[0] + 1
    children = child(edges, n)
    parents = parent(edges, n)
    m = len(edges)

    ordered_edges = np.zeros(m, dtype=int)
    incidence_n = incidence.copy()

    tog = np.vstack((edges, parents, children, ordered_edges)).T
    k = 1

    while np.any(tog[:, 3] == 0):

        incidence_submatrix = incidence_n[:, top_ordering - 1]
        col_sums = np.sum(incidence_submatrix, axis=0)
        nodes_with_parents = np.where(col_sums > 0)[0]
        if len(nodes_with_parents) == 0:
            print("No nodes with parents. Exiting.")
            break
        node1 = top_ordering[nodes_with_parents[0]]

        condition = tog[:, 2] == node1
        par1 = tog[condition, 1]
        g = par1[par1 > 0]

        if len(g) == 0:
            print(f"No valid parents for node {node1}. Skipping.")
            break

        f1 = np.array([np.where(top_ordering == g_i)[0][0] for g_i in g])
        par2 = g[np.argmax(f1)]

        condition = (tog[:, 1] == par2) & (tog[:, 2] == node1)
        tog[condition, 3] = k
        k += 1

        matching_rows = np.where(condition)[0]

        for row in matching_rows:
            edge_row = parents[row] - 1
            edge_col = children[row] - 1

            incidence_n[edge_row, edge_col] = 0

        tog[condition, 1] = 0

    result = np.vstack((edges, parents, children, tog[:, 3])).T
    return result


def cpdag(incidence):
    """
    :param incidence: Incidence/adjacency matrix
    :return: [edge, parent,
    child, position of the edge in the order, type of the edge (1=compelled; -1=reversible)]
    """
    z = order_edges(incidence)
    new_mat = np.column_stack((z, np.zeros(z.shape[0])))
    n_mat = new_mat[np.argsort(new_mat[:, 3])]
    vec = np.zeros(z.shape[0])

    while np.any(vec == 0):  # while there are unlabeled edges
        if len(vec) > 1:  # if there are at least 2 edges
            first = np.where(n_mat[:, 4] == 0)[0][0]  # first edge labeled "unknown" (0)
            parent1 = int(n_mat[first, 1]) - 1  # Convert to 0-based index
            child1 = int(n_mat[first, 2]) - 1  # Convert to 0-based index
            comp1 = n_mat[np.where((n_mat[:, 2] == parent1 + 1) & (n_mat[:, 4] == 1)), 1]  # Convert to 0-based index
        else:
            first = np.where(n_mat[:, 4] == 0)[0]  # first edge labeled "unknown" (0)
            parent1 = int(n_mat[first, 1]) - 1  # Convert to 0-based index
            child1 = int(n_mat[first, 2]) - 1  # Convert to 0-based index
            comp1 = np.array([])  # empty array for no compelled edges

        for j in np.atleast_1d(comp1).flat:  # Ensure j is iterated as scalars
            j = int(j) - 1  # Convert to 0-based index
            if incidence[j, child1] == 0:  # if w is not a parent of the child
                n_mat[first, 4] = 1  # label x -> y compelled
                n_mat[np.where(n_mat[:, 2] == child1 + 1), 4] = 1  # Convert back to 1-based index for comparison
                vec[first] = 1
                vec[np.where(n_mat[:, 2] == child1 + 1)] = 1
                break
            elif incidence[j, child1] != 0:  # if w is a parent of the child
                idx = np.where((n_mat[:, 1] == j + 1) & (n_mat[:, 2] == child1 + 1))  # Convert back to 1-based index
                n_mat[idx, 4] = 1  # label w -> y compelled
                vec[idx] = 1

        if len(vec) > 1 and n_mat[first, 4] == 0:  # if still unknown
            moep = n_mat[np.where((n_mat[:, 2] == child1 + 1) & (n_mat[:, 1] != parent1 + 1)), 1].astype(int).flatten()

            if len(moep) > 0:
                for o in moep:  # Loop over individual parents of the child, ensure o is an integer
                    o = int(o) - 1  # Convert to 0-based index
                    if incidence[o, parent1] == 0:  # Convert to 0-based index
                        vec[first] = 1
                        vec[np.where((n_mat[:, 2] == child1 + 1) & (n_mat[:, 4] == 0))] = 1
                        n_mat[first, 4] = 1  # label x -> y compelled
                        n_mat[np.where((n_mat[:, 2] == child1 + 1) & (
                                n_mat[:, 4] == 0)), 4] = 1  # label all "unknown" edges incident into y compelled
                        break
                    elif np.all(incidence[moep.astype(int) - 1, parent1] != 0):  # Convert to 0-based index
                        vec[first] = -1
                        vec[np.where((n_mat[:, 2] == child1 + 1) & (n_mat[:, 4] == 0))] = -1
                        n_mat[first, 4] = -1  # label x -> y reversible
                        n_mat[np.where((n_mat[:, 2] == child1 + 1) & (
                                n_mat[:, 4] == 0)), 4] = -1  # label all "unknown" edges incident into y reversible
            else:
                vec[first] = -1
                vec[np.where((n_mat[:, 2] == child1 + 1) & (n_mat[:, 4] == 0))] = -1
                n_mat[first, 4] = -1  # label x -> y reversible
                n_mat[np.where((n_mat[:, 2] == child1 + 1) & (
                        n_mat[:, 4] == 0)), 4] = -1  # label all "unknown" edges incident into y reversible

        if len(vec) == 1:
            n_mat[0, 4] = -1  # label x -> y reversible
            vec[0] = -1

    return n_mat


def create_pseudoincidence_matrix(edge_matrix, N):
    """
    :param edge_matrix: cpdag()
    :param N: Number of nodes in the DAG
    :return: CPDAG in readable form
    """
    pseudoincidence_matrix = np.zeros((N, N), dtype=int)

    for edge in edge_matrix:
        source = int(edge[1]) - 1
        target = int(edge[2]) - 1
        direction = int(edge[4])

        if direction == 1:
            pseudoincidence_matrix[source, target] = 1
        elif direction == -1:
            pseudoincidence_matrix[source, target] = -1
            pseudoincidence_matrix[target, source] = -1

    return pseudoincidence_matrix


def compute_shd(cpdag1, cpdag2):
    """
    :param cpdag1: CPDAG1
    :param cpdag2: CPDAG2
    :return: Structural Hamming Distance between cpdag1 and cpdag2
    """
    cpdag1 = np.array(cpdag1)
    cpdag2 = np.array(cpdag2)
    N = cpdag1.shape[0]

    if cpdag1.shape != cpdag2.shape:
        raise ValueError("The two CPDAG matrices must have the same dimensions.")

    shd = 0
    for i in range(N):
        for j in range(i + 1, N):
            if i != j:
                pair_a = (cpdag1[i, j], cpdag1[j, i])
                pair_b = (cpdag2[i, j], cpdag2[j, i])
                if pair_a != pair_b:
                    shd += 1

    return shd


def dag_coupling(A, B):
    """
    :param A: DAG A
    :param B: DAG B
    :return: Hamming Distance between A and B
    """
    return np.sum(A != B)


def skeleton_coupling(A, B):
    """
    :param A: DAG A
    :param B: DAG B
    :return: Hamming distance between A and B
    """
    A_bi = A + np.transpose(A)
    B_bi = B + np.transpose(B)
    A_upper = list(A_bi[np.triu_indices(A_bi.shape[0])])
    B_upper = list(B_bi[np.triu_indices(B_bi.shape[0])])
    return sum(i != j for i, j in zip(A_upper, B_upper))


def precompute_xi(N, fan_in):
    """
    :param N: Number of nodes
    :param fan_in: Fan-in
    :return: Xi matrix Eq. (2.70) from SAKI Algorithm (2.1)
    """
    xi = []
    for k in range(N):
        test = []
        for f in range(fan_in + 1):
            for j in range(min(k, f) + 1):
                mismatch = math.comb(k, j) * math.comb(N - 1 - k, f - j)
                lt = [(k - j) + (f - j)] * mismatch
                test.extend(lt)
        test.sort()
        xi.append(test)

    return xi


def Gibbs_model_CPDAG(p, beta_k, fan_in, M_hyp, M_k, prec_shd, same_hyp, dictio):
    """
    :param p: Xi matrix Eq. (2.70)
    :param beta_k: Hyperparameter \beta_k
    :param M_hyp: Hypernetwork
    :param M_k: DAG \M_k
    :param prec_shd: Meta-optimizaiton)
    :param same_hyp: Meta-optimization
    :param dictio: Meta-optimization
    :return: Eq. (2.80)
    """
    if same_hyp:
        nom = np.exp(-beta_k * prec_shd)
    else:
        new1 = create_pseudoincidence_matrix(cpdag(M_k), M_k.shape[0])
        SHD = compute_shd(new1, M_hyp)
        nom = np.exp(-beta_k * SHD)

    Z = 1
    N = M_k.shape[0]

    for Mhyp_key, Mhyp_L in dictio.items():
        if np.array_equal(np.array(Mhyp_key), M_hyp):
            for n in range(N):
                current_l = Mhyp_L[n]
                trial = sum(np.exp(-beta_k * l) for l in current_l)
                Z = Z * trial
            return nom / Z, dictio
        else:
            continue

    L = []

    for n in range(N):
        k = np.count_nonzero(M_hyp[:, n] == 1)
        l = np.count_nonzero(M_hyp[:, n] == -1)
        if l == 0:
            xi_k = p[k]
            trial_test = sum(np.exp(-beta_k * el) for el in xi_k)
        else:
            cargo = []
            for j in range(k, min(fan_in + 1, l + k + 1)):
                cargo.append(p[j])
            cargo_sum = np.sum(cargo, axis=0)
            xi_k = [x * (1 / (min(fan_in + 1 - k, l + 1))) for x in cargo_sum]
            trial_test = sum((np.exp(-beta_k * el) for el in xi_k))
        L.append(xi_k)
        Z = Z * trial_test
    dictio[tuple(map(tuple, M_hyp))] = L
    return nom / Z, dictio


def Gibbs_model_DAG(p, beta_k, M_hyp, M_k, prec_shd, same_hyp, dictio):
    """
    :param p: Xi matrix Eq. (2.70)
    :param beta_k: Hyperparameter \beta_k
    :param M_hyp: Hypernetwork
    :param M_k: DAG \M_k
    :param prec_shd: Meta-optimizaiton)
    :param same_hyp: Meta-optimization
    :param dictio: Meta-optimization
    :return: Eq. (2.79)
    """
    if same_hyp:
        nom = np.exp(-beta_k * prec_shd)
    else:
        SHD = dag_coupling(M_k, M_hyp)
        nom = np.exp(-beta_k * SHD)

    Z = 1
    N = M_k.shape[0]

    for Mhyp_key, Mhyp_L in dictio.items():
        if np.array_equal(np.array(Mhyp_key), M_hyp):
            for n in range(N):
                current_l = Mhyp_L[n]
                trial = sum(np.exp(-beta_k * l) for l in current_l)
                Z = Z * trial
            return nom / Z, dictio
        else:
            continue
    L = []
    for n in range(N):
        k = np.count_nonzero(M_hyp[:, n] == 1)
        xi_k = p[k]
        trial_test = sum(np.exp(-beta_k * el) for el in xi_k)
        L.append(xi_k)
        Z = Z * trial_test

    dictio[tuple(map(tuple, M_hyp))] = L

    return nom / Z, dictio


def Gibbs_model_SKELETON(p, beta_k, fan_in, M_hyp, M_k, prec_shd, same_hyp, dictio):
    """
    :param p: Xi matrix Eq. (2.70)
    :param beta_k: Hyperparameter \beta_k
    :param M_hyp: Hypernetwork
    :param M_k: DAG \M_k
    :param prec_shd: Meta-optimizaiton)
    :param same_hyp: Meta-optimization
    :param dictio: Meta-optimization
    :return: Eq. (2.81)
    """
    if same_hyp:
        nom = np.exp(-beta_k * prec_shd)
    else:
        SHD = skeleton_coupling(M_k, M_hyp)
        nom = np.exp(-beta_k * SHD)

    Z = 1
    N = M_k.shape[0]
    saki = M_hyp + np.transpose(M_hyp)

    for Mhyp_key, Mhyp_L in dictio.items():
        if np.array_equal(np.array(Mhyp_key), M_hyp):
            for n in range(N):
                current_l = Mhyp_L[n]
                trial = sum(np.exp(-beta_k * l) for l in current_l)
                Z = Z * trial
            return nom / Z, dictio
        else:
            continue
    L = []

    for n in range(N):
        k = np.count_nonzero(saki[:, n] == 1)
        if k != 0:
            cargo = []
            for j in range(min(fan_in + 1, k + 1)):
                cargo.append(p[j])
            cargo_sum = np.sum(cargo, axis=0)
            xi_k = [x * (1 / (min(fan_in + 1, k + 1))) for x in cargo_sum]
            trial_test = sum((np.exp(-beta_k * el) for el in xi_k))
        else:
            xi_k = p[k]
            trial_test = sum((np.exp(-beta_k * el) for el in xi_k))
        L.append(xi_k)
        Z = Z * trial_test
    dictio[tuple(map(tuple, M_hyp))] = L
    return nom / Z, dictio


def logBGe(Data, incidence, v=1, a=None, mu=None, T_0=None):
    """
    :param Data: Dataset
    :param incidence: Incidence/adjacency matrix
    :param v: Scaling parameter Sec. 2.4 and Sec. 3.1
    :param a: Degrees of freedom Sec. 2.4 and Sec. 3.1
    :param mu: Mean vector Sec. 2.4 and Sec. 3.1
    :param T_0: Covariance Sec. 2.4 and Sec. 3.1
    :return: BGe score
    """
    n, m = Data.shape

    if a is None:
        a = n + 2

    if mu is None:
        mu = np.zeros(n)

    if T_0 is None:
        T_0 = np.eye(n) * 0.5

    cov_t_Data = np.cov(Data, bias=False)
    mean_Data = np.mean(Data, axis=1)
    T_m = T_0 + (m - 1) * cov_t_Data + ((v * m) / (v + m)) * np.outer(mu - mean_Data, mu - mean_Data)

    def c_function(N, A):
        indices = np.arange(1, N + 1)
        fact = -gammaln((A + 1 - indices) / 2)
        product = np.sum(fact) - ((A * N) / 2) * np.log(2) - ((N * (N - 1)) / 4) * np.log(np.pi)
        return product

    P_local_num = np.zeros(n)
    P_local_den = np.zeros(n)

    for j in range(n):
        n_nodes = np.where(incidence[:, j] == 1)[0]
        len_n_nodes = len(n_nodes)
        combined_nodes = np.sort(np.concatenate((n_nodes, [j])))
        T0_combined = T_0[np.ix_(combined_nodes, combined_nodes)]
        Tm_combined = T_m[np.ix_(combined_nodes, combined_nodes)]
        det_T0_combined = np.linalg.det(T0_combined)
        det_Tm_combined = np.linalg.det(Tm_combined)

        if det_T0_combined <= 0 or det_Tm_combined <= 0:
            raise ValueError(f"Non-positive determinant encountered for node {j}.")

        log_det_T0_combined = np.log(det_T0_combined)
        log_det_Tm_combined = np.log(det_Tm_combined)

        P_local_num[j] = (
                (-(len_n_nodes + 1) * m / 2) * np.log(2 * np.pi)
                + ((len_n_nodes + 1) / 2) * np.log(v / (v + m))
                + c_function(len_n_nodes + 1, a)
                - c_function(len_n_nodes + 1, a + m)
                + (a / 2) * log_det_T0_combined
                - ((a + m) / 2) * log_det_Tm_combined
        )

        if len_n_nodes > 0:
            sorted_n_nodes = np.sort(n_nodes)
            T0_n_nodes = T_0[np.ix_(sorted_n_nodes, sorted_n_nodes)]
            Tm_n_nodes = T_m[np.ix_(sorted_n_nodes, sorted_n_nodes)]

            det_T0_n_nodes = np.linalg.det(T0_n_nodes)
            det_Tm_n_nodes = np.linalg.det(Tm_n_nodes)

            if det_T0_n_nodes <= 0 or det_Tm_n_nodes <= 0:
                raise ValueError(f"Non-positive determinant encountered for node {j} in denominator.")

            log_det_T0_n_nodes = np.log(det_T0_n_nodes)
            log_det_Tm_n_nodes = np.log(det_Tm_n_nodes)

            P_local_den[j] = (
                    (-(len_n_nodes) * m / 2) * np.log(2 * np.pi)
                    + (len_n_nodes / 2) * np.log(v / (v + m))
                    + c_function(len_n_nodes, a)
                    - c_function(len_n_nodes, a + m)
                    + (a / 2) * log_det_T0_n_nodes
                    - ((a + m) / 2) * log_det_Tm_n_nodes
            )
        else:
            P_local_den[j] = 0.0

    log_bge = np.sum(P_local_num) - np.sum(P_local_den)

    return log_bge


def edge_addition(A):
    """
    :param A: Incidence/adjacency matrix
    :return: Allowed edge additions
    """
    E = np.zeros_like(A)
    N = A.shape[0]
    for k in range(1, N + 1):
        E += np.linalg.matrix_power(A, k)

    ancestor_matrix = (E > 0).astype(int)
    ones = np.ones((N, N), dtype=int)
    identity = np.eye(N, dtype=int)

    allowed_additions = (ones - identity - A - ancestor_matrix.T)
    positions = np.argwhere(allowed_additions == 1)
    new_matrices = []

    for position in positions:
        new_matrix = A.copy()
        new_matrix[position[0], position[1]] = 1
        new_matrices.append(new_matrix)

    return new_matrices


def edge_reversal(A):
    """
    :param A: Incidence/adjacency matrix
    :return: Allowed edge reversals
    """
    all_edges = np.argwhere(A == 1)
    N = A.shape[0]
    new_matrices = []

    for edge in all_edges:
        new_matrix = A.copy()
        new_matrix[edge[0], edge[1]] = 0
        E = np.zeros_like(A)
        for k in range(1, N + 1):
            E += np.linalg.matrix_power(new_matrix, k)
        new_matrix[edge[1], edge[0]] = 1
        ancestor_matrix = (E > 0).astype(int)
        if (ancestor_matrix[edge[0], edge[1]]).T == 0:
            new_matrices.append(new_matrix)

    return new_matrices


def neighbors_edge_removal(A):
    """
    :param A: Incidence/adjacency matrix
    :return: Allowed edge removals
    """
    neighbors = [A]
    rows, cols = A.shape

    for i in range(rows):
        for j in range(cols):
            if A[i][j] == 1:
                new_matrix = A.copy()
                new_matrix[i][j] = 0
                neighbors.append(new_matrix)

    return neighbors


def has_more_than_N_parents(matrix, fan_in):
    """
    :param matrix: Incidence/adjacency matrix
    :param fan_in: Fan-in
    :return: Boolean
    """
    rows, cols = matrix.shape
    for col in range(cols):
        parent_count = np.sum(matrix[:, col])
        if parent_count > fan_in:
            return True
    return False


def filter_DAGs_with_parent_limit(neighbors_list, fan_in):
    """
    :param neighbors_list: All neighbor DAGs
    :param fan_in: Fan-in
    :return: Neighbor DAGs satisfying fan-in
    """
    return [matrix for matrix in neighbors_list if not has_more_than_N_parents(matrix, fan_in)]


def neighbors_set_wrapper(A, fan_in):
    """
    :param A: Incidence/adjacency matrix
    :param fan_in: Fan-in
    :return: Neighbor DAGs
    """
    deleted = neighbors_edge_removal(A)
    added = edge_addition(A)
    reversed = edge_reversal(A)
    all_neig = deleted + added + reversed

    return filter_DAGs_with_parent_limit(all_neig, fan_in)


def reflection(beta, max_value):
    """
    :param beta: Hyperparameter
    :param max_value: MAX in Eq. (3.7)
    :return: Reflection Eq. (3.7)
    """
    if beta < 0:
        beta = -beta
    elif beta > max_value:
        beta = 2 * max_value - beta
    else:
        pass
    return beta


def MCMC(parentsets, init_model, datas, init_hypnet, N, MAX, burn_in, L, fan_in, variation):
    """
    :param parentsets: Xi matrix Eq. (2.70)
    :param init_model: Initial DAG(s)
    :param datas: Data
    :param init_hypnet: Initial hypernetwork
    :param N: Length of the MCMC
    :param MAX: MAX parameter from Eq. (3.7)
    :param burn_in: Burn-in phase
    :param L: L parameter from Sec. 3.1
    :param fan_in: Fan-in
    :param variation: Type of coupling scheme
    :return: MCMC samples
    """
    M = len(datas)
    hyps = [init_hypnet]
    params = [[] for _ in range(M)]
    models = [[] for _ in range(M)]

    for m in range(M):
        models[m].append(init_model)
        params[m].append(random.uniform(0, MAX))
    Z_dict = {}

    last_models = [None] * M
    last_params = [None] * M

    for n in range(N):
        print(n)
        flag = True
        for m in range(M):
            # ------ MCMC for the models ------
            M_old = models[m][-1]
            neighbors_M_old = neighbors_set_wrapper(M_old, fan_in)
            M_new = random.choice(neighbors_M_old)
            neighbors_M_new = neighbors_set_wrapper(M_new, fan_in)

            new1_new = create_pseudoincidence_matrix(cpdag(M_new), M_new.shape[0])
            new1_old = create_pseudoincidence_matrix(cpdag(M_old), M_old.shape[0])

            if variation == "CPDAG_couple":
                new2_new = create_pseudoincidence_matrix(cpdag(hyps[-1]), hyps[-1].shape[0])
                SHD_new = compute_shd(new1_new, new2_new)
                nom_new = np.exp(-params[m][-1] * SHD_new)
                SHD_old = compute_shd(new1_old, new2_new)
                nom_old = np.exp(-params[m][-1] * SHD_old)

            if variation == "SKELETON_couple":
                SHD_new = skeleton_coupling(M_new, hyps[-1])
                nom_new = np.exp(-params[m][-1] * SHD_new)
                SHD_old = skeleton_coupling(M_old, hyps[-1])
                nom_old = np.exp(-params[m][-1] * SHD_old)

            if variation == "DAG_couple":
                SHD_new = dag_coupling(M_new, hyps[-1])
                nom_new = np.exp(-params[m][-1] * SHD_new)
                SHD_old = dag_coupling(M_old, hyps[-1])
                nom_old = np.exp(-params[m][-1] * SHD_old)

            if np.array_equal(new1_new, new1_old):
                A = (nom_new * len(neighbors_M_old)) / (nom_old * len(neighbors_M_new))
            else:
                A = (np.exp(logBGe(datas[m], M_new)) * nom_new * len(neighbors_M_old)) / (
                        np.exp(logBGe(datas[m], M_old)) * nom_old * len(neighbors_M_new))

            u = random.uniform(0, 1)
            if A >= 1 or A >= u:
                models[m].append(M_new)
                last_models[m] = M_new
                shd = SHD_new
            else:
                models[m].append(M_old)
                last_models[m] = M_old
                shd = SHD_old

            # ------ MCMC for the hyperparameters ------
            b_old = params[m][-1]
            b_new = reflection(random.uniform(b_old - L / 2, b_old + L / 2), MAX)

            if variation == "CPDAG_couple":
                gibbs_new, dc = Gibbs_model_CPDAG(parentsets, b_new, fan_in, new2_new, models[m][-1], shd, flag, Z_dict)
                gibbs_old, dc_up = Gibbs_model_CPDAG(parentsets, b_old, fan_in, new2_new, models[m][-1], shd, flag, dc)

            if variation == "SKELETON_couple":
                gibbs_new, dc = Gibbs_model_SKELETON(parentsets, b_new, fan_in, hyps[-1], models[m][-1], shd, flag,
                                                     Z_dict)
                gibbs_old, dc_up = Gibbs_model_SKELETON(parentsets, b_old, fan_in, hyps[-1], models[m][-1], shd, flag,
                                                        dc)

            if variation == "DAG_couple":
                gibbs_new, dc = Gibbs_model_DAG(parentsets, b_new, hyps[-1], models[m][-1], shd, flag, Z_dict)
                gibbs_old, dc_up = Gibbs_model_DAG(parentsets, b_old, hyps[-1], models[m][-1], shd, flag, dc)

            Z_dict = dc_up
            A_b = gibbs_new / gibbs_old

            if A_b >= 1 or A_b >= u:
                params[m].append(b_new)
                last_params[m] = b_new
            else:
                params[m].append(b_old)
                last_params[m] = b_old

        Mhyp_old = hyps[-1]
        neighbors_Mhyp_old = neighbors_set_wrapper(Mhyp_old, fan_in)
        Mhyp_new = random.choice(neighbors_Mhyp_old)
        neighbors_Mhyp_new = neighbors_set_wrapper(Mhyp_new, fan_in)

        W = len(neighbors_Mhyp_old) / len(neighbors_Mhyp_new)
        flag = False
        shd = 0
        Ahyp = 1
        if variation == "CPDAG_couple":
            Mhyp_newCPDAG = create_pseudoincidence_matrix(cpdag(Mhyp_new), Mhyp_new.shape[0])
            Mhyp_oldCPDAG = create_pseudoincidence_matrix(cpdag(Mhyp_old), Mhyp_old.shape[0])

        for i in range(M):
            if variation == "CPDAG_couple":
                gibbs_hyp_new, dc_hyp = Gibbs_model_CPDAG(parentsets, params[i][-1], fan_in, Mhyp_newCPDAG,
                                                          models[i][-1], shd, flag,
                                                          Z_dict)
                gibbs_hyp_old, dc_hyp_up = Gibbs_model_CPDAG(parentsets, params[i][-1], fan_in, Mhyp_oldCPDAG,
                                                             models[i][-1], shd, flag,
                                                             dc_hyp)

            if variation == "SKELETON_couple":
                gibbs_hyp_new, dc_hyp = Gibbs_model_SKELETON(parentsets, params[i][-1], fan_in, Mhyp_new, models[i][-1],
                                                             shd, flag,
                                                             Z_dict)
                gibbs_hyp_old, dc_hyp_up = Gibbs_model_SKELETON(parentsets, params[i][-1], fan_in, Mhyp_old,
                                                                models[i][-1], shd, flag,
                                                                dc_hyp)

            if variation == "DAG_couple":
                gibbs_hyp_new, dc_hyp = Gibbs_model_DAG(parentsets, params[i][-1], Mhyp_new, models[i][-1], shd, flag,
                                                        Z_dict)
                gibbs_hyp_old, dc_hyp_up = Gibbs_model_DAG(parentsets, params[i][-1], Mhyp_old, models[i][-1], shd,
                                                           flag,
                                                           dc_hyp)

            Z_dict = dc_hyp_up
            Ahyp_loc = (gibbs_hyp_new / gibbs_hyp_old)
            Ahyp *= Ahyp_loc
        Ahyp = Ahyp * W
        u = random.uniform(0, 1)
        if Ahyp >= 1 or Ahyp >= u:
            hyps.append(Mhyp_new)
            last_hyp = Mhyp_new
        else:
            hyps.append(Mhyp_old)
            last_hyp = Mhyp_old

        if n < burn_in:
            for m in range(M):
                models[m] = [last_models[m]]
                params[m] = [last_params[m]]
            hyps = [last_hyp]

        # dictionary compression
        if len(Z_dict) > 1:
            keys = list(Z_dict.keys())[-1:]
            Z_dict = {key: Z_dict[key] for key in keys}

    return models, params, hyps


def highlight_entries(ax, gold_standard_network, mean_matrix):
    """
    :param ax: sns heatmap
    :param gold_standard_network: True DAG
    :param mean_matrix: Mean (averaged) network
    :return: Highlighted entries w.r.t. True DAG
    """
    rows, cols = gold_standard_network.shape
    total_entries = 0
    green_correct = 0
    red_correct = 0

    for i in range(rows):
        for j in range(cols):
            if gold_standard_network[i, j] == 1:
                rect = plt.Rectangle((j, i), 1, 1, linewidth=2, edgecolor='green', facecolor='none')
                ax.add_patch(rect)

                if mean_matrix[i, j] > 0.3:
                    green_correct += 1

                rect_transposed = plt.Rectangle((i, j), 1, 1, linewidth=2, edgecolor='red', facecolor='none')
                ax.add_patch(rect_transposed)

                if mean_matrix[j, i] > 0.3:
                    red_correct += 1

                total_entries += 1


def plot_roc_curves_all(mean_cpdags, total_mean_cpdag, gold_standard_network, path):
    """
    :param mean_cpdags: Mean CPDAG samples
    :param total_mean_cpdag: Mean mean CPDAG
    :param gold_standard_network: True DAG
    :param path: path to the output file
    :return: ROC evaluations
    """
    gold_standard_skeleton = np.maximum(gold_standard_network, gold_standard_network.T)
    y_true_skeleton = gold_standard_skeleton.flatten()

    gold_standard_cpdag = create_pseudoincidence_matrix(cpdag(gold_standard_network), gold_standard_network.shape[0])
    gold_standard_cpdag[gold_standard_cpdag == -1] = 1
    y_true_cpdag = gold_standard_cpdag.flatten()

    file_path = os.path.join(path, "results.txt")

    with open(file_path, "w") as file:
        file.write("SKELETON ROC\n")
        skeleton_auc_values = []

        for idx, mean_cpdag in enumerate(mean_cpdags):
            mean_skeleton = np.maximum(mean_cpdag, mean_cpdag.T)
            y_pred_skeleton = mean_skeleton.flatten()

            fpr, tpr, _ = roc_curve(y_true_skeleton, y_pred_skeleton)
            roc_auc = auc(fpr, tpr)
            skeleton_auc_values.append(f"{roc_auc:.2f},")

        file.write(" ".join(skeleton_auc_values) + "\n")

        total_mean_skeleton = np.maximum(total_mean_cpdag, total_mean_cpdag.T)
        y_pred_total_skeleton = total_mean_skeleton.flatten()
        fpr_total, tpr_total, _ = roc_curve(y_true_skeleton, y_pred_total_skeleton)
        roc_auc_total = auc(fpr_total, tpr_total)
        file.write(f"{roc_auc_total:.2f}\n")

        file.write("CPDAG ROC\n")
        cpdag_auc_values = []

        for idx, mean_cpdag in enumerate(mean_cpdags):
            y_pred_cpdag = mean_cpdag.flatten()

            fpr, tpr, _ = roc_curve(y_true_cpdag, y_pred_cpdag)
            roc_auc = auc(fpr, tpr)
            cpdag_auc_values.append(f"{roc_auc:.2f},")

        file.write(" ".join(cpdag_auc_values) + "\n")

        y_pred_total_cpdag = total_mean_cpdag.flatten()
        fpr_total, tpr_total, _ = roc_curve(y_true_cpdag, y_pred_total_cpdag)
        roc_auc_total_cpdag = auc(fpr_total, tpr_total)
        file.write(f"{roc_auc_total_cpdag:.2f}\n")

    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    colors = ["blue", "orange", "green", "red", "purple"]
    markers = ["o", "v", "s", "^", "D"]

    axes[0].set_title("RAF Skeleton ROC Curves")
    for idx, mean_cpdag in enumerate(mean_cpdags):
        mean_skeleton = np.maximum(mean_cpdag, mean_cpdag.T)
        y_pred_skeleton = mean_skeleton.flatten()
        fpr, tpr, _ = roc_curve(y_true_skeleton, y_pred_skeleton)
        roc_auc = auc(fpr, tpr)

        axes[0].plot(fpr, tpr, label=f"Dataset {idx + 1} AUC = {roc_auc:.2f}",
                     color=colors[idx % len(colors)], lw=1, linestyle="--", marker=markers[idx % len(markers)],
                     markevery=10)

    axes[0].plot(fpr_total, tpr_total, label=f"Total Mean CPDAG AUC = {roc_auc_total:.2f}",
                 color="black", linestyle="-", linewidth=2)

    axes[0].plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')
    axes[0].set_xlim([0.0, 1.0])
    axes[0].set_ylim([0.0, 1.05])
    axes[0].set_xlabel('False Positive Rate')
    axes[0].set_ylabel('True Positive Rate')
    axes[0].legend(loc="lower right")

    axes[1].set_title("RAF CPDAG ROC Curves")
    for idx, mean_cpdag in enumerate(mean_cpdags):
        y_pred_cpdag = mean_cpdag.flatten()
        fpr, tpr, _ = roc_curve(y_true_cpdag, y_pred_cpdag)
        roc_auc = auc(fpr, tpr)

        axes[1].plot(fpr, tpr, label=f"Dataset {idx + 1} AUC = {roc_auc:.2f}",
                     color=colors[idx % len(colors)], lw=1, linestyle="--", marker=markers[idx % len(markers)],
                     markevery=10)

    axes[1].plot(fpr_total, tpr_total, label=f"Total Mean CPDAG AUC = {roc_auc_total_cpdag:.2f}",
                 color="black", linestyle="-", linewidth=2)

    axes[1].plot([0, 1], [0, 1], color='navy', lw=0.5, linestyle='--')
    axes[1].set_xlim([0.0, 1.0])
    axes[1].set_ylim([0.0, 1.05])
    axes[1].set_xlabel('False Positive Rate')
    axes[1].set_ylabel('True Positive Rate')
    axes[1].legend(loc="lower right")

    plt.tight_layout()
    plt.savefig(os.path.join(path, "ROC_all_combined.png"))
    plt.close()


def plot_pr_curves_all(mean_cpdags, total_mean_cpdag, gold_standard_network, path):
    """
    :param mean_cpdags: Mean CPDAG samples
    :param total_mean_cpdag: Mean mean CPDAG
    :param gold_standard_network: True DAG
    :param path: path to the ouput file
    :return: PR evaluations
    """

    gold_standard_skeleton = np.maximum(gold_standard_network, gold_standard_network.T)
    y_true_skeleton = gold_standard_skeleton.flatten()

    gold_standard_cpdag = create_pseudoincidence_matrix(cpdag(gold_standard_network), gold_standard_network.shape[0])
    gold_standard_cpdag[gold_standard_cpdag == -1] = 1
    y_true_cpdag = gold_standard_cpdag.flatten()

    file_path = os.path.join(path, "results.txt")

    with open(file_path, "a") as file:
        file.write("SKELETON PR\n")
        skeleton_pr_auc_values = []

        for idx, mean_cpdag in enumerate(mean_cpdags):
            mean_skeleton = np.maximum(mean_cpdag, mean_cpdag.T)
            y_pred_skeleton = mean_skeleton.flatten()

            precision, recall, _ = precision_recall_curve(y_true_skeleton, y_pred_skeleton)
            pr_auc = auc(recall, precision)
            skeleton_pr_auc_values.append(f"{pr_auc:.2f},")

        file.write(" ".join(skeleton_pr_auc_values) + "\n")

        total_mean_skeleton = np.maximum(total_mean_cpdag, total_mean_cpdag.T)
        y_pred_total_skeleton = total_mean_skeleton.flatten()
        precision_total, recall_total, _ = precision_recall_curve(y_true_skeleton, y_pred_total_skeleton)
        pr_auc_total = auc(recall_total, precision_total)
        file.write(f"{pr_auc_total:.2f}\n")

        file.write("CPDAG PR\n")
        cpdag_pr_auc_values = []

        for idx, mean_cpdag in enumerate(mean_cpdags):
            y_pred_cpdag = mean_cpdag.flatten()

            precision, recall, _ = precision_recall_curve(y_true_cpdag, y_pred_cpdag)
            pr_auc = auc(recall, precision)
            cpdag_pr_auc_values.append(f"{pr_auc:.2f},")

        file.write(" ".join(cpdag_pr_auc_values) + "\n")

        y_pred_total_cpdag = total_mean_cpdag.flatten()
        precision_totalx, recall_totalx, _ = precision_recall_curve(y_true_cpdag, y_pred_total_cpdag)
        pr_auc_total_cpdag = auc(recall_totalx, precision_totalx)
        file.write(f"{pr_auc_total_cpdag:.2f}\n")

    fig, axes = plt.subplots(1, 2, figsize=(16, 8))

    colors = ["blue", "orange", "green", "red", "purple"]
    markers = ["o", "v", "s", "^", "D"]

    axes[0].set_title("RAF Skeleton Precision-Recall Curves")
    for idx, mean_cpdag in enumerate(mean_cpdags):
        mean_skeleton = np.maximum(mean_cpdag, mean_cpdag.T)
        y_pred_skeleton = mean_skeleton.flatten()
        precision, recall, _ = precision_recall_curve(y_true_skeleton, y_pred_skeleton)
        pr_auc = auc(recall, precision)

        axes[0].plot(recall, precision, label=f"Dataset {idx + 1} AUC = {pr_auc:.2f}",
                     color=colors[idx % len(colors)], lw=1, linestyle="--", marker=markers[idx % len(markers)],
                     markevery=10)

    axes[0].plot(recall_total, precision_total, label=f"Total Mean CPDAG AUC = {pr_auc_total:.2f}",
                 color="black", linestyle="-", linewidth=2)

    axes[0].set_xlim([0.0, 1.0])
    axes[0].set_ylim([0.0, 1.05])
    axes[0].set_xlabel('Recall')
    axes[0].set_ylabel('Precision')
    axes[0].legend(loc="lower right")

    axes[1].set_title("RAF CPDAG Precision-Recall Curves")
    for idx, mean_cpdag in enumerate(mean_cpdags):
        y_pred_cpdag = mean_cpdag.flatten()
        precision, recall, _ = precision_recall_curve(y_true_cpdag, y_pred_cpdag)
        pr_auc = auc(recall, precision)

        axes[1].plot(recall, precision, label=f"Dataset {idx + 1} AUC = {pr_auc:.2f}",
                     color=colors[idx % len(colors)], lw=1, linestyle="--", marker=markers[idx % len(markers)],
                     markevery=10)

    axes[1].plot(recall_totalx, precision_totalx, label=f"Total Mean CPDAG AUC = {pr_auc_total:.2f}",
                 color="black", linestyle="-", linewidth=2)

    axes[1].set_xlim([0.0, 1.0])
    axes[1].set_ylim([0.0, 1.05])
    axes[1].set_xlabel('Recall')
    axes[1].set_ylabel('Precision')
    axes[1].legend(loc="lower right")

    plt.tight_layout()
    plt.savefig(os.path.join(path, "PR_all_combined.png"))
    plt.close()


def plot_mcmc_distributions(models, betas, hypnets, path, R):
    """
    :param models: CPDAG samples
    :param betas: Hyperparameters
    :param hypnets: Hypernetwork samples
    :param path: path to the ouput file
    :return: Various statistics related to mean networks of all kind
    """
    total_mean_model = None
    total_mean_cpdag = None
    gold_standard_network = R

    K = len(models)
    mean_cpdags = []

    for dataset_index in range(K):
        models_samples = np.array(models[dataset_index])
        mean_model = np.mean(models_samples, axis=0)

        if total_mean_model is None:
            total_mean_model = mean_model
        else:
            total_mean_model += mean_model

        cpdag_samples = [create_pseudoincidence_matrix(cpdag(j), j.shape[0]) for j in models_samples]

        for i in range(len(cpdag_samples)):
            cpdag_samples[i][cpdag_samples[i] == -1] = 1

        cpdag_samples = np.array(cpdag_samples)
        mean_cpdag = np.mean(cpdag_samples, axis=0)
        mean_cpdags.append(mean_cpdag.copy())

        plt.figure(figsize=(6, 6))
        ax_hypnet = sns.heatmap(mean_cpdag, annot=True, cmap="Reds", cbar=True)
        plt.title(f"Averaged Bidirected CPDAG Posterior Probabilities (Dataset {dataset_index + 1})")
        highlight_entries(ax_hypnet, gold_standard_network, mean_cpdag)
        plt.savefig(os.path.join(path, f"cpdag{dataset_index + 1}.png"))
        plt.close()

        if total_mean_cpdag is None:
            total_mean_cpdag = mean_cpdag
        else:
            total_mean_cpdag += mean_cpdag

    total_mean_model /= K
    total_mean_cpdag /= K

    plot_roc_curves_all(mean_cpdags, total_mean_cpdag, gold_standard_network, path)
    plot_pr_curves_all(mean_cpdags, total_mean_cpdag, gold_standard_network, path)

    hypnets_samples = np.array(hypnets)

    hyp_samples = []
    for j in hypnets_samples:
        hyp_samples.append(create_pseudoincidence_matrix(cpdag(j), j.shape[0]))
    hyp_samples = [np.where(matrix == -1, 1, matrix) for matrix in hyp_samples]
    hyp_samples = np.array(hyp_samples)
    mean_hypcpdag = np.mean(hyp_samples, axis=0)

    plt.figure(figsize=(6, 6))
    ax_hypcpdag = sns.heatmap(mean_hypcpdag, annot=True, cmap="Blues", cbar=True)
    plt.title(f"Bidirected Hypernetwork CPDAG Posterior Probabilities")
    highlight_entries(ax_hypcpdag, gold_standard_network, mean_hypcpdag)
    plt.savefig(os.path.join(path, "HYPCPDAG.png"))
    plt.close()

    plt.figure(figsize=(6, 6))
    ax_total_model = sns.heatmap(total_mean_model, annot=True, cmap="Blues", cbar=True)
    plt.title("Averaged DAG")
    highlight_entries(ax_total_model, gold_standard_network, total_mean_model)
    plt.savefig(os.path.join(path, "averagedDAG.png"))
    plt.close()

    plt.figure(figsize=(6, 6))
    ax_total_cpdag = sns.heatmap(total_mean_cpdag, annot=True, cmap="Blues", cbar=True)
    plt.title("Averaged Bidirected CPDAG")
    highlight_entries(ax_total_cpdag, gold_standard_network, total_mean_cpdag)
    plt.savefig(os.path.join(path, "averagedCPDAG.png"))
    plt.close()


def plotMCMC(init_hypnet, M, R, I, path, fan_in, N, burn_in, MAX, L, variation, R_len):
    """
    Helper function that helps to launch the MCMC. For inputs see config.py.
    """

    R_datasets = []
    N_M = M.shape[0]
    xi_sets = precompute_xi(N_M, fan_in)
    for _ in range(I):
        R_datasets.append(R_data(R, R_len))

    models, betas, hypnets = MCMC(parentsets=xi_sets, init_model=M, datas=R_datasets,
                                  init_hypnet=init_hypnet, N=N, MAX=MAX,
                                  burn_in=burn_in, L=L, fan_in=fan_in, variation=variation)
    size = len(models[0])
    params = [[] for _ in range(I)]

    for n in range(size):
        for i in range(I):
            params[i].append(betas[i][n])

    fig2, axes = plt.subplots(nrows=1, ncols=I, figsize=(15, 4), sharey=True)
    colors = sns.color_palette("tab10", I)

    for i in range(I):
        param_values = params[i]

        axes[i].hist(param_values, bins=20, color=colors[i], edgecolor='black', alpha=0.7)
        axes[i].set_title(f"Parameter Histogram (Dataset {i + 1})", fontsize=10)
        axes[i].set_xlabel("Parameter Value", fontsize=8)
        axes[i].set_ylabel("Frequency", fontsize=8)
        axes[i].tick_params(axis='both', which='major', labelsize=8)

    plt.tight_layout(pad=2.0)
    plt.savefig(os.path.join(path, "parameter_histograms.png"))
    plt.close(fig2)

    fig3, axes3 = plt.subplots(nrows=1, ncols=I, figsize=(15, 4), sharex=True)

    for i in range(I):
        param_values = params[i]
        axes3[i].plot(range(len(param_values)), param_values, label=f"Dataset {i + 1}", color=colors[i])
        axes3[i].set_title(f"Parameter Trace (Dataset {i + 1})", fontsize=10)
        axes3[i].set_xlabel("Iteration", fontsize=8)
        axes3[i].set_ylabel("Parameter Value", fontsize=8)
        axes3[i].tick_params(axis='both', which='major', labelsize=8)
        axes3[i].set_ylim(0, 30)

    plt.tight_layout(pad=2.0)
    plt.savefig(os.path.join(path, "parameter_traces.png"))
    plt.close(fig3)

    plot_mcmc_distributions(models, betas, hypnets, path, R)


if __name__ == "__main__":
    """
    Launcher function. Provided the configuration file CONFIG, it launches the MCMC.
    """
    parameters = CONFIG
    plotMCMC(*parameters)
